---
title: 'HarvardX: PH125.9x Final Project'
author: "By Philip J Brown,  pjbMit@pjb3.com"
date: "6/16/2019"
output: pdf_document
---
```{r initialize, echo=FALSE, include=FALSE}

source("real_ml_script.R")
knitr::opts_chunk$set(echo = FALSE, include=TRUE,cache =FALSE)

#Load libraries, installing as necessary
if(!require(jsonlite)) install.packages("jsonlite", repos = repos)
if(!require(tidyverse)) install.packages("tidyverse", repos = repos)
if(!require(lubridate)) install.packages("lubridate", repos = repos)
if(!require(R.utils)) install.packages("R.utils", repos = repos)
if(!require(caret)) install.packages("caret", repos = repos)
if(!require(corrplot)) install.packages("corrplot", repos = repos)  #provides corrplot visualizarion
if(!require(kernlab)) install.packages("kernlab", repos = repos)  #provides 'rvmLinear' model method
if(!require(knitr)) install.packages("knitr", repos = repos)
if(!require(broom)) install.packages("broom", repos = repos)

#Read in the R script, so that we can
#Execute portions named with:   ## @ knitr chunk_name
# with entries of the form: ```{r chunk_name}
read_chunk('real_ml_script.R')

#knitr options are:
#include= the output, or not
#cache= the result, or not
#eval= the code, or not
#echo= the code to knitr doc, or not

```


**HarvardX Data Science Capstone Class** 
PH125.9x (2T2018)

* Student: Philip Brown 
* email: Phil@pjb3.com
* github: https://github.com/pjbMit


**RealML** - *A Real Estate Machine Learning Project*

This is RealML, a real estate machine learning project and report created by Philip J Brown (pjbMit@pjb3.com) as the final capstone project for the Data Science Certificate program offered by HarvardX: PH125.9x from edx.org.

# Part 1) EXECUTIVE SUMMARY

The goal of this project is to utilize data analysis and modelling skills to a create machine learning engine and this report as the final exercise in completing the 9 course Data Science Certificate program offered by HarvardX through edx.org.  

For my project I chose to use machine learning techniques to build a *RealML,a real estate sales price prediction engine.*
More specifically, I wanted to answer this question:

> **Can I reasonably predict the resale price of residential condominum and single family real estate within a five mile radius of *[Fairlington Villages (link)](http://www.fairlingtonvillages.com/)*, the condominum development in Arlington Virginia that I call home?**

Through this project, I am able to demonstrate examples of data identificaton and acquisition, data wrangling and cleansing, data analysis, modeling and machine learning techniques, data presentation and data visualization and report generation and presentation.  The project was built by acquiring and analyzing more than 20,000 reports of current real estate sales within the stated five mile radius for residential properties that sold for at least \$5,000 but less than \$1,000,000.  

After some research I was able to locate and curate live data for this project, so the basis for this report is real and impactful -- at least it is to me as home owner in this area.  * :-) 

The results of this analysis were very encouraging, and are included in the **results** section and the **conclusion** section, which are the last two sections in this report.

The mission was to locate, curate, wrangle and cleanse real data, and use it build a prediction engine that tries to predict sales prices so as to optimize the model for a low Root Mean Square Error (RMSE), defined as 
$$RMSE = \sqrt{\Sigma_{i=1}^{n}{\frac{(actual_i -predicted_i)^2}{N}}}$$

This project is intended to highlight some of the skills acquired throughout the courses in this program.  All programming was done in R Code using RStudio on MacBook Pro.  After acquiring and processing the data, the real work began!

In addition to this **executive summary**, this report also includes a **methods and analysis section**, a **results section** and a **conclusion section**.

Key files for this project have been uploaded and stored on my git hub page at  
[github.com/pjbMit/real_estate_project](https://github.com/pjbMit/real_estate_project "My Git Hub RealML Project"). The three main files for this project are listed below, and can be viewed on git hub -- The file names are also links:

* [real_ml_script.R *(link)*](https://github.com/pjbMit/real_estate_project/blob/master/real_ml_script.R "RealML R Script")

* [real_ml_report.Rmd *(link)*](https://github.com/pjbMit/real_estate_project/blob/master/real_ml_report.Rmd "RealML Report Rmd file")

* [real_ml_report.pdf *(link)*](https://github.com/pjbMit/real_estate_project/blob/master/real_ml_report.pdf "RealML Report pdf file")


Additionally, a gzip'd version of the data file is on github at:

* [realml_data_file.github.json.gz *(link)*](https://github.com/pjbMit/real_estate_project/blob/master/realml_data_file.github.json.gz "RealML Data File on github")



# Part 2) METHODS AND ANALYSIS

The project was created in the RStudio environment 
using Rstudio Version 1.1.442 
on a Macintosh; Intel Mac OS X 10_14_5

R version 3.5.1 (2018-07-02)  
nickname       Feather Spray

All code was written in R and executed in RStudio.

Here are the methods and techniques used.

Data was downloaded from AttomData.com, a commercial data provider, using their RESTful API and
an apikey that is needed in order to get data.  Sales data was queried from their API, and
results were downloaded 10,000 rows at a time.

Working code to download from Atom Data is included in the .Rmd script, but the function call is commented out.

you may run it by calling **download_and_save_web_data()** which is a function that will connect to the Restful API service, query for the next 10,000 matching rows, and loop five times, downloading up to 50,000 rows of the most recent real estate sales data.  Once the data is filtered, you are likely to end up with about 1/2 as many records that in scope for this project.

The data was then filtered to remove property types that aren't residential condos or homes.  Data wrangling skills were used to massage and clean up the data.  A fairly clean version of the data with Factors is within the factorData object.

Here are some summary results produced from the factorData object:

```{r after_setup1, echo=FALSE, include=TRUE}

#Show "Resale' for "SFR" and "CONDOMINIUM
new_old_sale
```


```{r after_setup2, echo=TRUE, include=TRUE}
resale_type_subtype
```



# Part 3) RESULTS 

After downloaing the raw data, we had sales information covering the following sales date range:
```{r summary_date_range, echo=TRUE, include=TRUE, eval=TRUE, cache=FALSE}
```

We found additional filter criteria to help us cleanse the data.
For example, shown below is the  data after filtering to show just the property types and subtypes of interest,
which had the effect of removing commercial sales, industrial sales, and other data that is out of scope
for this project.

```{r summary_proptype_subtype, echo=TRUE, include=TRUE, cache=FALSE}
```

After examining the data, we saw that a little data-cleansing house keeping was in order.
We found that about 10% of the data had zero listed for bedrooms, yet those units had
about 1400 sqft on average (mean), thus the zero bedrooms was clearly an error.
We removed these rows along with two unneeded colums.

```{r cleanse_data, echo=TRUE, include=TRUE}
```

After cleansing the data, here's a grouped summary showing the data by year and property type:

```{r summary_by_year}
```

That's a decent amount of *current* data.  If I can just tun the model appropriately, we should be good to go.

Now it's time to look at a summary() of the data to get a better feel some the individual data attributes.

 ```{r summary_head}
 ```

The text is useful, but graphs are better...
```{r summary_histogram}
```

The bar plots are a little jagged.  Lets get a different perspective by smoothing out the graphs using a density plot. This helps us better visualize the data to see if we have a binomial distribution, or other anomally.

```{r summary_histogram2, echo=TRUE, include=TRUE, fig.height=4, fig.width=4}
```
The SqFt, in particular has a long tail towards some very large houses.  This *could* be a problem for some models, and might be a source of model error.  (Update -- after looking into the rather large model errors, I believe that the skew towards large million dollar houses, has the effect either through the model or through my sampling methods, of biasing the model to over-estimated.  I suspect that other models could compensate, especially one the uses regularization.  I ran almost a dozen different models.  Quite truthfully, something made my models *exceptionally* slow, which kept me from working more with different models.)

Next, we looked the remaining columns to see how they correlated, to see if we can
remove any columns that are highly correlated.

```{r correlation_plot }
```

The data is highly correlated.  We use the calculation below to determine
which attribute, if any, we should consider removing to see if our models improve...

```{r attribute_correation_removal, echo=TRUE, include=TRUE}

```
 and the answer turns out to be baths.  So, as part of the data modelling, we should experiment with models both with and with baths in them, to see if removing baths helps the model perform better.

### Modelling

Now it's time to explore our data models.
Let's try a mix of non-linear models, and see how we do.
Note that we enabled parallel processing in the caret package by loading the **doMC** package and library.
(If it gives you trouble, comment it out on lines 43 and 44 in the script. For me, it made some modes wayyyyyy faster!!!)

Here are the models from our first run.


```{r model_run1, echo=TRUE, include=TRUE}
print("This is the first run")
```

And here are the results of the output.
```{r echo=TRUE, include=TRUE}

```

So, the average home cost is about 540K, and our best RSME is about 140K... that's not very good. We need to look into the data and find out what to change to do better.

Let's add "lat","yearbuilt","zip" to the model, and rerun it both with and without "bath" as a paramter.
The results are below...


I'm still not happy with the results.  Let's look at a box-and-whisker plot for more insight.  The outliers will deserve more attention, and a box and whisker plot can help us look for skews in the data.

```{r}

# boxplots for each attribute
par(mfrow=c(2,7))
for(i in c(1:7,13)) {
  boxplot(myTrain[,i], main=names(myTrain)[i])
}
```

```{r knitr new_chunk, echo=TRUE}


```
Time to drill down into the sources of the errors...


```{r look_at_errors}

```
Putting the errors on a logarithmic scale, helps clarify the skew.  Our error distribution is skewed, and we have a small number of very large over-estimates.  For example, the data above shows that one of the y's is \$385K, but two of the models predicted \$875K and \$894k!.  

```{r show_log_plot, echo=TRUE}

```

# Part 4) CONCLUSION
Massive over-predictions are skewing the model.  It will take more investigation and research to build a better model.  I had initially used the log function with the createDataPartition argument to impement binning so that we would get better represeentation from the whole range of values.  I thought that this might have skewed the model, but when I re-ran it without binning, the results weren't much better.

Next, I'd like to try some of the models the implement regularization, since I suspect that a small number of very expensive homes are skewing the model.  
```{r}
mfrow=c(2,1)
histogram(myTrain$price)
plot(density(myTrain[,"price"]), main="price")

```
As the two price plots above show, it seems like the prices themselves have an underlying bi-nomial distribution.   There are the main group of houses centered around \$500K, plus there appears to be another group of homes centered around \$850K.  Armed with this insight, the search will contiue!  I suspect that some of the machine learning algorithms may handle this situation with ease.  Certainly more experimentation and research is called for.  Plus, for models that have tunining paramters, some manual tuning may be required.  Note that I used a grid for some of the models, and took advantage of the grid tuning inherint with specifying the model and a grid.  

I have demonstrated a desire to pick a challenging topic -- namely, getting real, live data for *current* home sales near my home.  The search, learnign the api, getting access to the data, and coding to pull the data are all part of a good data science exercise.  It would have been easier to have selected a pre-cleansed data set of off kaggle, and followed a scripted approach to building a model, but I decided to be more adventurous.  I know that with time, experimentation and maybe even collaboration with others in the field, that I will be able to improve the model results substantially.  Ensemble models, and ones with feature identification could all help.  Nonetheless, I consider this assignment a success, in that I pulled together many of the skills and lessons tauht in this program to produce running code, data analysis, graphics and some insight.  Yes, I realize that the machine learning algorithm needs more work, but I *did* implement several, and worked through them.  The project was very useful.  It helped clarify many of the subtlties that you can only learn by *doing*.

One last note.  For quite some time, my machine ran exceptionally slow on models.  I eventually re-wrote my model code, and changed to using the x,y format instead of the formula format. There must have been some error in my previous code, because suddenly, these models ran orders of magnitude faster.  This isn't a bad thing -- it's all part of the journey, and the reason why I enrolled in the program -- namely, to learn by doing.  With those barriers behind me, I look forward to my next challenge.  

I also wanted to highlight that I installed the *doMC* package, and configured it to use 8 parallel threads for some the carret packages... what a differnce multi-core computing makes!   

I believe that this assignment sucessfully fulfilled the requirements.  To answer my initial question, right now I would have to say "No!" -- I have not yet built a machine learning model that is accurate enough to be impressive, but, my road has just begun, and I *did* build a machine learning model.  It will be an itereative process, and a fun one. Cheers!

```{r signature, echo=FALSE,include=TRUE}
#Note that `echo = FALSE`  was added to the code chunk to prevent printing this code out of the generated pdf.
print("Thanks for checking this out!  pjbMit@pjb3.com  :-)")
```



