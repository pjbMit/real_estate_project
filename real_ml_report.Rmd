---
title: 'HarvardX: PH125.9x Final Project'
author: "By Philip J Brown,  pjbMit@pjb3.com"
date: "6/16/2019"
output: pdf_document
---
```{r initialize, include=FALSE, cache=FALSE, eval=TRUE, echo=FALSE}
require(knitr)
#Read in the R script, so that we can
#Execute portions named with:   ## @knitr chunk_name
# with entries of the form: ```{r chunk_name}
read_chunk('real_ml_script.R')
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, eval=TRUE, INCLUDE=FALSE)
runningInScript <- FALSE
#knitr options are:
#include= the output, or not
#cache= the result, or not
#evaluate= the code, or not
#echo= the code to knitr doc, or not
```



**HarvardX Data Science Capstone Class** 
PH125.9x (2T2018)

* Student: Philip Brown 
* email: Phil@pjb3.com
* github: https://github.com/pjbMit


**RealML** - *A Real Estate Machine Learning Project*

This is RealML, a real estate machine learning project and report created by Philip J Brown (pjbMit@pjb3.com) as the final capstone project for the Data Science Certificate program offered by HarvardX: PH125.9x from edx.org.

# Part 1) EXECUTIVE SUMMARY

The goal of this project is to utilize data analysis and modelling skills to a create machine learning engine and this report as the final exercise in completing the 9 course Data Science Certificate program offered by HarvardX through edx.org.  

For my project I chose to use machine learning techniques to build a *RealML,a real estate sales price prediction engine.*
More specifically, I wanted to answer this question:

> **Can I reasonably predict the resale price of residential condominum and single family real estate within a five mile radius of *[Fairlington Villages (link)](http://www.fairlingtonvillages.com/)*, the condominum development in Arlington Virginia that I call home?**

Through this project, I am able to demonstrate examples of data identificaton and acquisition, data wrangling and cleansing, data analysis, modeling and machine learning techniques, data presentation and data visualization and report generation and presentation.  The project was built by acquiring and analyzing more than 20,000 reports of current real estate sales within the stated five mile radius for residential properties that sold for at least \$5,000 but less than \$1,000,000.  

After some research I was able to locate and curate live data for this project, so the basis for this report is real and impactful -- at least it is to me as home owner in this area.  * :-) 

The results of this analysis were very encouraging, and are included in the **results** section and the **conclusion** section, which are the last two sections in this report.

The mission was to locate, curate, wrangle and cleanse real data, and use it build a prediction engine that tries to predict sales prices so as to optimize the model for a low Root Mean Square Error (RMSE), defined as 
$$RMSE = \sqrt{\Sigma_{i=1}^{n}{\frac{(actual_i -predicted_i)^2}{N}}}$$

This project is intended to highlight some of the skills acquired throughout the courses in this program.  All programming was done in R Code using RStudio on MacBook Pro.  After acquiring and processing the data, the real work began!

In addition to this **executive summary**, this report also includes a **methods and analysis section**, a **results section** and a **conclusion section**.

Key files for this project have been uploaded and stored on my git hub page at  
[github.com/pjbMit/real_estate_project](https://github.com/pjbMit/real_estate_project "My Git Hub RealML Project"). The three main files for this project are listed below, and can be viewed on git hub -- The file names are also links:

* [real_ml_script.R *(link)*](https://github.com/pjbMit/real_estate_project/blob/master/real_ml_script.R "RealML R Script")

* [real_ml_report.Rmd *(link)*](https://github.com/pjbMit/real_estate_project/blob/master/real_ml_report.Rmd "RealML Report Rmd file")

* [real_ml_report.pdf *(link)*](https://github.com/pjbMit/real_estate_project/blob/master/real_ml_report.pdf "RealML Report pdf file")


Additionally, a gzip'd version of the data file is on github at:

* [realml_data_file.github.json.gz *(link)*](https://github.com/pjbMit/real_estate_project/blob/master/realml_data_file.github.json.gz "RealML Data File on github")




# Part 2) METHODS AND ANALYSIS

The project was created in the RStudio environment 
using Rstudio Version 1.1.442 
on a Macintosh; Intel Mac OS X 10_14_5

R version 3.5.1 (2018-07-02)  
nickname       Feather Spray

All code was written in R and executed in RStudio.

Here are the methods and techniques used.

Data was downloaded from AttomData.com, a commercial data provider, using their RESTful API and
an apikey that is needed in order to get data.  Sales data was queried from their API, and
results were downloaded 10,000 rows at a time.

The data was then filtered to remove property types that aren't residential condos or homes.

<!-- Reads and executs the script portion taged with the knitr label set_up_code -->
```{r set_up_code, echo=FALSE, include=FALSE, eval=TRUE}
```


```{r define_functions, echo=FALSE,include=FALSE,eval=TRUE}
```

# Part 3) RESULTS {#Best-Results}

After downloaing the raw data, we had sales information covering the following sales date range:
```{r summary_date_range, echo=FALSE, include=TRUE,eval=TRUE, cache=TRUE}
```

We found additional filter criteria to help us cleanse the data.
For example, shown below is the  data after filtering to show just the property types and subtypes of interest,
which had the effect of removing commercial sales, industrial sales, and other data that is out of scope
for this project.

```{r summary_proptype_subtype, echo=FALSE, include=TRUE, cache=TRUE}
```

After examining the data, we saw that a little data-cleansing house keeping was in order.
We found that about 10% of the data had zero listed for bedrooms, yet those units had
about 1400 sqft on average (mean), thus the zero bedrooms was clearly an error.
We removed these rows along with two unneeded colums.

```{r cleanse_data}
```

After cleansing the data, here's a grouped summary showing the data by year and property type:

<!-- consider changing to a bar chart -->
```{r summary_by_year, echo=FALSE, include=TRUE, cache=FALSE}
```

Now it's time to look at some the individual data attributes.

```{r summary_head, echo=FALSE}
```


Next, we looked the remaining columns to see how they correlated, so that we can
remove any columns that are highly correlated.



TODO
* Data was ...
* After these models were evaluated, we looked at variability, and attempted to add genre to models using several standard models available through the **caret package** and applied techniques such as cross-validation. While we examined these models, and made multiple attempts to improve the results, none of the techniques tried improved upon the best results that were previously used.

 
-- General approach:

For many approaches, I  first tried working on a very small data set, just to get the code working,
then I re-ran the on a medium sized data set, and then when I was satisfied, then I processed the full training set.

Similarly, initially I did NOT do full cross-validation, but once the model was built and the code was working, I enabled cross validation and other ML techniques.

Additionally, being sensitive to computation times, I wrote code and used global variables to enable saving daa and objects containing intermediate results as files on the local file system.  By changing the values of these logial variable from TRUE to FALSE, or vice-versa, I was able to re-run code without having to repeat some of the more lengthy processing or repeatedly downloading and cleansing the same data.

--set up

Set up libraries and enable multi-core processing 
for some of the operations used by the caret package.  
Because I have an 8 core processor, for calcuations that can 
utilize the parallel processing features, this script runs ***substantially** faster.






TODO


# Part 4) CONCLUSION

The model results were promising, as can be seen by the output from rmse_results.


I discovered a library and options to set to enable multi-core parallel processing for some of the algorithms in the **caret** package, and this technique helped tremendously, as I was able to span 8 R-sessions that ran in parallel to process some of the algorithms.  

Ultimately, the best results that I obtained were a **RMSE of TODO** which was obtained ffrom the final model.  This was deemed satisfactory based on goals and scope of this project.  Of course, if you plan to move nearby, please do your own due dilligence before purchasing a home -- while I wanted to choose an impactful and relevant project, this project was created primarily for didactic purposes. 

*(See the output below which shows the best results obtained.)*

```{r Best_Results}

# The estimates that minimize this can be found similarly to what we did above. 
# Here we use cross-validation to pick a  lambda
if(runningInScript){
}

```




```{r signature, echo=FALSE,include=TRUE}
#Note that `echo = FALSE`  was added to the code chunk to prevent printing this code out of the generated pdf.
print("Thanks for checking this out!  pjbMit@pjb3.com  :-)")
```
